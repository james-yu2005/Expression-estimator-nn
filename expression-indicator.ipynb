{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39603,"sourceType":"datasetVersion","datasetId":31050,"isSourceIdPinned":false}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-13T20:44:56.287825Z","iopub.execute_input":"2024-09-13T20:44:56.288213Z","iopub.status.idle":"2024-09-13T20:45:01.570835Z","shell.execute_reply.started":"2024-09-13T20:44:56.288174Z","shell.execute_reply":"2024-09-13T20:45:01.569762Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"emotion_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\ndf=pd.read_csv('/kaggle/input/facial-expression/fer2013.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-13T20:45:01.572689Z","iopub.execute_input":"2024-09-13T20:45:01.573212Z","iopub.status.idle":"2024-09-13T20:45:07.964533Z","shell.execute_reply.started":"2024-09-13T20:45:01.573174Z","shell.execute_reply":"2024-09-13T20:45:07.963396Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T20:45:07.965750Z","iopub.execute_input":"2024-09-13T20:45:07.966085Z","iopub.status.idle":"2024-09-13T20:45:07.987102Z","shell.execute_reply.started":"2024-09-13T20:45:07.966049Z","shell.execute_reply":"2024-09-13T20:45:07.986189Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   emotion                                             pixels     Usage\n0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n5        2  55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...  Training\n6        4  20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...  Training\n7        3  77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...  Training\n8        3  85 84 90 121 101 102 133 153 153 169 177 189 1...  Training\n9        2  255 254 255 254 254 179 122 107 95 124 149 150...  Training","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emotion</th>\n      <th>pixels</th>\n      <th>Usage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4</td>\n      <td>20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3</td>\n      <td>85 84 90 121 101 102 133 153 153 169 177 189 1...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>255 254 255 254 254 179 122 107 95 124 149 150...</td>\n      <td>Training</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"m,n = df.shape\nprint(m,n)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T20:45:07.989103Z","iopub.execute_input":"2024-09-13T20:45:07.989396Z","iopub.status.idle":"2024-09-13T20:45:07.994138Z","shell.execute_reply.started":"2024-09-13T20:45:07.989364Z","shell.execute_reply":"2024-09-13T20:45:07.993153Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"35887 3\n","output_type":"stream"}]},{"cell_type":"code","source":"# Process data from FER2013\ndef preprocess_data(data):\n    labels = data['emotion'].values\n    \n    # Takes string values and forms an array with float32 data points of size 48 by 48\n    images = data['pixels'].apply(lambda x: np.array(x.split(), dtype='float32').reshape(48, 48))\n    \n    # Stack images into a 3D array (num_samples, 48, 48) -> (35887, 48, 48)\n    images = np.stack(images.values)  \n    \n    # Add a channel dimension for grayscale -> (num_samples, channels, 48, 48) -> (35887, 1, 48, 48)\n    # Pytorch models expect in format of (batch_size, channels, height, width)\n    images = np.expand_dims(images, axis=1)  \n    \n    # Normalize pixel values to [0, 1]\n    images /= 255.0  \n    return images, labels\n\n# Split data into train and test datasets\ntrain_images, train_labels = preprocess_data(df[:30000])\ntest_images, test_labels = preprocess_data(df[30000:])","metadata":{"execution":{"iopub.status.busy":"2024-09-13T20:45:07.995741Z","iopub.execute_input":"2024-09-13T20:45:07.996123Z","iopub.status.idle":"2024-09-13T20:45:26.923630Z","shell.execute_reply.started":"2024-09-13T20:45:07.996079Z","shell.execute_reply":"2024-09-13T20:45:26.922593Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n# # Number of images to display\n# num_images = 5\n# # Define the number of columns for the grid layout\n# num_cols = 3\n# # Calculate the number of rows needed\n# num_rows = (num_images + num_cols - 1) // num_cols  # Ceiling division\n\n# # Create a figure with subplots\n# fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 4, num_rows * 3))\n\n# # Flatten the axes array for easier iteration\n# axes = axes.flatten()\n\n# for i in range(num_images):\n#     ex_emotion = emotion_map[labels[i]]  # Provides format of (1,48,48)\n#     ex_image = images[i].squeeze()  # Squeezes first dimension to become (48,48)\n\n#     # Displaying with pyplot from matplotlib (do not need to convert back into 0-255, can keep as 0-1)\n#     axes[i].imshow(ex_image, cmap='gray')\n#     axes[i].set_title(f'Emotion Label: {ex_emotion}')\n#     axes[i].axis('off')\n\n# # Turn off the axes for unused plots\n# for j in range(num_images, len(axes)):\n#     axes[j].axis('off')\n\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-13T20:45:26.925200Z","iopub.execute_input":"2024-09-13T20:45:26.926100Z","iopub.status.idle":"2024-09-13T20:45:26.931886Z","shell.execute_reply.started":"2024-09-13T20:45:26.926052Z","shell.execute_reply":"2024-09-13T20:45:26.930626Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Define Dataset\nclass FER2013Dataset(Dataset):\n    def __init__(self, images, labels):\n        self.images = torch.tensor(images, dtype=torch.float32)  # Convert numpy array to torch tensor\n        self.labels = torch.tensor(labels, dtype=torch.long)    # Convert labels to long tensor for classification\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        label = self.labels[idx]\n        return image, label\n    \nbatch_size = 64\nshuffle = True\n\n# Create dataset instance\ntrain_dataset = FER2013Dataset(train_images, train_labels)\ntest_dataset = FER2013Dataset(test_images, test_labels)\n\n# Create data loader\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T20:45:26.932973Z","iopub.execute_input":"2024-09-13T20:45:26.933280Z","iopub.status.idle":"2024-09-13T20:45:27.083989Z","shell.execute_reply.started":"2024-09-13T20:45:26.933233Z","shell.execute_reply":"2024-09-13T20:45:27.082925Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Define the model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n        self.fc2 = nn.Linear(128, 7)  # 7 emotions\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 64 * 12 * 12)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nmodel = SimpleCNN().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T20:45:27.085317Z","iopub.execute_input":"2024-09-13T20:45:27.085741Z","iopub.status.idle":"2024-09-13T20:45:27.264885Z","shell.execute_reply.started":"2024-09-13T20:45:27.085684Z","shell.execute_reply":"2024-09-13T20:45:27.263843Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\nlearning_rate = 0.001\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i, (images,labels) in enumerate(train_dataloader):\n        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        if (i + 1) % 100 == 0:\n            print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{i + 1}/{len(train_dataloader)}], Loss: {running_loss / (i + 1):.4f}')\n    \n    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_dataloader):.4f}')\n\n# Save the model\n# torch.save(model.state_dict(), 'fer2013_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-09-13T20:45:55.933310Z","iopub.execute_input":"2024-09-13T20:45:55.933723Z","iopub.status.idle":"2024-09-13T20:46:16.795509Z","shell.execute_reply.started":"2024-09-13T20:45:55.933667Z","shell.execute_reply":"2024-09-13T20:46:16.794461Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch [1/10], Batch [100/469], Loss: 1.7867\nEpoch [1/10], Batch [200/469], Loss: 1.7465\nEpoch [1/10], Batch [300/469], Loss: 1.7149\nEpoch [1/10], Batch [400/469], Loss: 1.6908\nEpoch [1/10], Loss: 1.6767\nEpoch [2/10], Batch [100/469], Loss: 1.5448\nEpoch [2/10], Batch [200/469], Loss: 1.5355\nEpoch [2/10], Batch [300/469], Loss: 1.5262\nEpoch [2/10], Batch [400/469], Loss: 1.5129\nEpoch [2/10], Loss: 1.5053\nEpoch [3/10], Batch [100/469], Loss: 1.4321\nEpoch [3/10], Batch [200/469], Loss: 1.4152\nEpoch [3/10], Batch [300/469], Loss: 1.4078\nEpoch [3/10], Batch [400/469], Loss: 1.4052\nEpoch [3/10], Loss: 1.4038\nEpoch [4/10], Batch [100/469], Loss: 1.3242\nEpoch [4/10], Batch [200/469], Loss: 1.3332\nEpoch [4/10], Batch [300/469], Loss: 1.3344\nEpoch [4/10], Batch [400/469], Loss: 1.3297\nEpoch [4/10], Loss: 1.3275\nEpoch [5/10], Batch [100/469], Loss: 1.2576\nEpoch [5/10], Batch [200/469], Loss: 1.2654\nEpoch [5/10], Batch [300/469], Loss: 1.2723\nEpoch [5/10], Batch [400/469], Loss: 1.2733\nEpoch [5/10], Loss: 1.2702\nEpoch [6/10], Batch [100/469], Loss: 1.2039\nEpoch [6/10], Batch [200/469], Loss: 1.2010\nEpoch [6/10], Batch [300/469], Loss: 1.2055\nEpoch [6/10], Batch [400/469], Loss: 1.2049\nEpoch [6/10], Loss: 1.2077\nEpoch [7/10], Batch [100/469], Loss: 1.1371\nEpoch [7/10], Batch [200/469], Loss: 1.1422\nEpoch [7/10], Batch [300/469], Loss: 1.1421\nEpoch [7/10], Batch [400/469], Loss: 1.1464\nEpoch [7/10], Loss: 1.1494\nEpoch [8/10], Batch [100/469], Loss: 1.0769\nEpoch [8/10], Batch [200/469], Loss: 1.0812\nEpoch [8/10], Batch [300/469], Loss: 1.0871\nEpoch [8/10], Batch [400/469], Loss: 1.0836\nEpoch [8/10], Loss: 1.0869\nEpoch [9/10], Batch [100/469], Loss: 1.0240\nEpoch [9/10], Batch [200/469], Loss: 1.0259\nEpoch [9/10], Batch [300/469], Loss: 1.0233\nEpoch [9/10], Batch [400/469], Loss: 1.0278\nEpoch [9/10], Loss: 1.0270\nEpoch [10/10], Batch [100/469], Loss: 0.9435\nEpoch [10/10], Batch [200/469], Loss: 0.9497\nEpoch [10/10], Batch [300/469], Loss: 0.9615\nEpoch [10/10], Batch [400/469], Loss: 0.9664\nEpoch [10/10], Loss: 0.9699\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\n\n# Assuming you have a DataLoader for the test set\nmodel.eval()  # Set the model to evaluation mode\nall_labels = []\nall_preds = []\n\nwith torch.no_grad():\n    for images, labels in test_dataloader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        all_labels.extend(labels.cpu().numpy())\n        all_preds.extend(predicted.cpu().numpy())\n\naccuracy = accuracy_score(all_labels, all_preds)\nprint(f'Accuracy: {accuracy:.4f}')\n\nprint(classification_report(all_labels, all_preds))","metadata":{"execution":{"iopub.status.busy":"2024-09-13T20:46:18.999002Z","iopub.execute_input":"2024-09-13T20:46:19.000029Z","iopub.status.idle":"2024-09-13T20:46:19.878211Z","shell.execute_reply.started":"2024-09-13T20:46:18.999973Z","shell.execute_reply":"2024-09-13T20:46:19.876976Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Accuracy: 0.5167\n              precision    recall  f1-score   support\n\n           0       0.38      0.48      0.42       780\n           1       0.47      0.30      0.36        90\n           2       0.35      0.39      0.37       828\n           3       0.70      0.76      0.73      1468\n           4       0.40      0.33      0.36      1028\n           5       0.65      0.68      0.67       673\n           6       0.53      0.40      0.45      1020\n\n    accuracy                           0.52      5887\n   macro avg       0.50      0.48      0.48      5887\nweighted avg       0.52      0.52      0.51      5887\n\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'fer2013_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-09-13T20:45:27.615931Z","iopub.status.idle":"2024-09-13T20:45:27.616286Z","shell.execute_reply.started":"2024-09-13T20:45:27.616115Z","shell.execute_reply":"2024-09-13T20:45:27.616133Z"},"trusted":true},"execution_count":null,"outputs":[]}]}